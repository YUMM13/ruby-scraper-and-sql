## DX Coding Challenge
### Ruby Scraper
For this challenge, I was asked to create a Web Scraper in Ruby to harvest information about Vercel's repositories. I needed to get info about each repository, all pull requests, and their reviews.
I utilized 2 main Ruby libraries: HTTParty and Nokogiri. HTTParty is let me send GET requests for any information I needed and Nokogiri is a very useful HTTP parser. I used HTTParty to get the raw HTML for all 172 repositories, and Nokogiri to parse the HTML for the information I needed like the repository name, link, visibility, and whether it was archived or not.
To get the pull requests and reviews, I used the GitHub API to easily grab information about each pull request and review. I decided to use the API because I can quickly grab the info I need since it is in a JSON format as opposed to HTML, which has a more difficult parsing process. As a result, the code is easier to read and write. The GitHub API does require a Fine-grained Personal Access Token to be used, which I generated on my own and stored in a .env file. To use the script, you will need to provide your own since giving you my API key is not a good practice and can result in malicious acts being done under my name.
Once I grab the information that I need, I store all the information in a SQLite database. SQLite allows me to quickly spin up a database on my local machine. It is very useful in small projects like this one and for proof-of-concept projects where a full-sized database like Postgres is unnecessary. Every time scraper.rb is ran, the database drops all previous tables and creates new ones with fresh data. You can run the script by running "ruby scraper.rb" in your console while in the same directory as the scraper file.

### SQL Challenge
For this challenge, I was tasked with creating a single SQL query that would gather information from a large, pre-made database and display it as a table. This would function as a dashboard, allowing a user to view how well teams they have access to are doing at specific snapshots, and if any areas have improved or need improvement.
I used DBeaver as my database client since it can connect to numerous database types, including Postgres, and it has powerful ER diagram generators. When working with a database this large and complex, I find it easier to use ER diagrams to visualize the schema and see where foreign keys are to see connections. DBeaver lets me create an ER diagram for the schema and see connections between foreign keys, allowing me to create queries faster. 
My main strategy with this was to divide and conquer: create small queries that selected portions of the data and combine them together at the end to have my single large query. Some queries were simple, like getting the factor or the snapshot squad info, but some queries were more complex. For example, grabbing the hierarchy above the team utilized recursive queries, something I have never used before. After some trial and error, I got it working.
This strategy worked well for almost all of the queries. As of writing this (1:00 PM Saturday), I was unable to get the sentiment score and count of each value working correctly. Both are reporting inaccurate numbers, and I was unable to find the solution to the issue. I was also unable to get the query working for the entire table. I can select the information as shown in the example for a single row, but I was unable to get an entire table in the allotted time.
My individual queries for this problem are in "SQL_queries.sql" and the final answer that combines everything into one large query is in "SQL_query_final_ans.sql".
